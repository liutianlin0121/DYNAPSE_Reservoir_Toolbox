{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liutianlin/anaconda3/lib/python3.6/site-packages/brian2/core/variables.py:174: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return np.issubdtype(np.bool, self.dtype)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from brian2 import *\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.sparse\n",
    "from sklearn import linear_model\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "import scipy.signal\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/liutianlin/Desktop/Academics/MINDS/neuromorphic/reservoir_Demo/ECG/data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_Dynapse_data(coreID,neuronID,Tm):\n",
    "    core_id_array = []\n",
    "    with open(coreID, 'r') as f:\n",
    "        for line in f:\n",
    "            core_id_array.append(int(line))\n",
    "        core_id_array = np.array(core_id_array)\n",
    "    \n",
    "    neuron_id_array = []\n",
    "    with open(neuronID, 'r') as f:\n",
    "        for line in f:\n",
    "            neuron_id_array.append(int(line))\n",
    "        neuron_id_array = np.array(neuron_id_array)\n",
    "\n",
    "\n",
    "    time_array = []\n",
    "    with open(Tm, 'r') as f:\n",
    "        for line in f:\n",
    "            time_array.append(int(line))\n",
    "        time_array = np.array(time_array)\n",
    "    \n",
    "    neuron_id_array0=neuron_id_array[core_id_array==0]\n",
    "    time_array0=time_array[core_id_array==0]\n",
    "    pulse_ts = time_array0[neuron_id_array0==1]\n",
    "\n",
    "    t_min = np.min(time_array)\n",
    "    t_max = np.max(time_array)\n",
    "\n",
    "    unit_t = 10**(-6)*second\n",
    "        \n",
    "    neuron_id_array = (core_id_array-1)*256+neuron_id_array\n",
    "    reservoir_neurons = neuron_id_array[core_id_array!=0]\n",
    "    spike_times = time_array[core_id_array!=0]\n",
    "        \n",
    "    #plot(spike_times*unit_t, reservoir_neurons, '.')\n",
    "    #show()\n",
    "        \n",
    "    print(reservoir_neurons.shape)\n",
    "    print(spike_times.shape)\n",
    "    \n",
    "    return reservoir_neurons, spike_times\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_psc(spike_train, t, alpha = 4):\n",
    "    if spike_train.size == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        distance_vec = unit_t * (spike_train - t) / second\n",
    "        return np.sum(np.exp(alpha*distance_vec[distance_vec<0]))\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "def exp_smooth(spike_train, t_grid, alpha=2, inc = 1):\n",
    "    n_spike = len(spike_train)\n",
    "    n_t = len(t_grid)\n",
    "    i_spike = 0\n",
    "    i_t = 0\n",
    "    sig = 0\n",
    "    sig_list = []\n",
    "    sig_t =0\n",
    "    while i_t < n_t:\n",
    "        if i_spike >= n_spike:\n",
    "            delta_t = unit_t * (t_grid[i_t] - sig_t)/second\n",
    "            sig = sig/np.exp(alpha*delta_t)\n",
    "            sig_list.append(sig)\n",
    "            sig_t = t_grid[i_t]\n",
    "            i_t += 1\n",
    "        elif spike_train[i_spike] < t_grid[i_t]:\n",
    "            delta_t = unit_t * (spike_train[i_spike] - sig_t)/second\n",
    "            sig = sig/np.exp(alpha*delta_t)+inc\n",
    "            sig_t = spike_train[i_spike]\n",
    "            i_spike += 1\n",
    "        else:\n",
    "            delta_t = unit_t * (t_grid[i_t] - sig_t)/second\n",
    "            sig = sig/np.exp(alpha*delta_t)\n",
    "            sig_list.append(sig)\n",
    "            sig_t = t_grid[i_t]\n",
    "            i_t += 1\n",
    "    return sig_list   \n",
    "  \n",
    "    \n",
    "\n",
    "def smooth_spike_trains(spike_times, reservoir_neurons, nrBin, n_neurons = 3 *256, alpha=2.5, inc=1):\n",
    "    t_grid = np.linspace(spike_times[0], spike_times[-1], nrBin)\n",
    "    \n",
    "    activity_list = [exp_smooth(spike_times[reservoir_neurons==i], t_grid, alpha) for i in range(n_neurons) ]\n",
    "    return np.array(activity_list)\n",
    "\n",
    "\n",
    "\t\n",
    "def read_ECG(MatECG):\n",
    "\trate = scipy.io.loadmat(MatECG)\n",
    "\trate_key = list(rate.keys())\n",
    "\tecg_key = rate_key[3]\n",
    "\tecg_raw = rate[ecg_key]\n",
    "\treturn ecg_raw\n",
    "\n",
    "\n",
    "def read_target(trgtMat):\n",
    "\trate = scipy.io.loadmat(trgtMat)\n",
    "\trate_key = list(rate.keys())\n",
    "\ttrgt_key = rate_key[3]\n",
    "\ttrgt_raw = rate[trgt_key]\n",
    "\treturn trgt_raw\n",
    "\n",
    "\n",
    "def confusion_stat(confusion):\n",
    "\tTP = confusion[1,1]\n",
    "\tFN = confusion[1,0]\n",
    "\tFP = confusion[0,1]\n",
    "\tTN = confusion[0,0]\n",
    "\t#Accuracy \n",
    "\tACC = (TP + TN ) / (TP + FP + TN + FN)\n",
    "\t# sensitivity (recall)\n",
    "\tSEN = TP/(TP + FN)\n",
    "\t# precision\n",
    "\tPRE = TP/(TP + FP)\n",
    "\t# F1 score\n",
    "\tF1 = (2 * TP) / (2 * TP + FP + FN)\n",
    "\treturn ACC, SEN, PRE, F1\n",
    "    \n",
    "def binarize(y, thres):\n",
    "\ty_b = y\n",
    "\ty_b[y >= thres] = 1\n",
    "\ty_b[y < thres] = 0\n",
    "\treturn y_b\n",
    "\n",
    "\n",
    "def scan(out, nrBins):\n",
    "\n",
    "    width = int(len(out)/nrBins)\n",
    "\n",
    "    result = np.zeros(nrBins)\n",
    "\n",
    "    for i in np.arange(nrBins):\n",
    "        out_in_bin = out[i*width: i*width + width]\n",
    "        if np.count_nonzero(out_in_bin) == 0:\n",
    "            result[i] = 0\n",
    "        else :\n",
    "            result[i] = 1\n",
    "            \n",
    "    return result\n",
    "\n",
    "def get_rPeak_locs(rPeaks, scan_num):\n",
    "    rPeaks_down =  scan(rPeaks, scan_num)\n",
    "    rPeaks_down_diff = rPeaks_down[1:] - rPeaks_down[:-1]\n",
    "    return np.hstack([np.array([0]),(rPeaks_down_diff==1)])\n",
    "\n",
    "\n",
    "\n",
    "def better_predict(prdct, rPeaks_locs, avg_width, thr):\n",
    "    count = prdct[rPeaks_locs.astype(int)==1]\n",
    "    count[:] = 1\n",
    "    prdct_i = prdct[rPeaks_locs.astype(int)==1]\n",
    "    for i in range(avg_width-1):\n",
    "        ind_vec = np.hstack([np.array((i+1)*[0]), rPeaks_locs[:-i-1]])\n",
    "        length = prdct[ind_vec.astype(int)==1].shape[0]\n",
    "        prdct_i[:length] += binarize(prdct[ind_vec.astype(int)==1], thr)\n",
    "        count[:length] += 1\n",
    "        #plt.plot(prdct_i)    \n",
    "    return binarize(prdct_i/count, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "inp = 'ECG_119'\n",
    "trgt = 'Target119'\n",
    "ecg = read_ECG(inp)\n",
    "\n",
    "rPeaks = scipy.io.loadmat('Rpeaks_119')['Rpeaks'].flatten()[:324000]\n",
    "rPeaks_train = rPeaks[:2 * 108000] \n",
    "rPeaks_test = rPeaks[2 * 108000:]\n",
    "\n",
    "\n",
    "output = read_target(trgt)[:324000,0] # only first 15 min in this experiment\n",
    "\n",
    "# framesPerSecond = 2\n",
    "framesPerSecond = 9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_out_pre = output[:2 * 108000] \n",
    "train_out  = scan(train_out_pre, int(2 * 108000 * framesPerSecond/ 360))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_out_pre = output[2 * 108000:]\n",
    "test_out  = scan(test_out_pre, int( 108000 * framesPerSecond / 360))\n",
    "\n",
    "\n",
    "rPeaks_locs_train = get_rPeak_locs(rPeaks_train, int(2 * 108000 * 9 / 360))\n",
    "rPeaks_locs_test = get_rPeak_locs(rPeaks_test, int(108000 * 9 / 360))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3463259,)\n",
      "(3463259,)\n"
     ]
    }
   ],
   "source": [
    "trainDataName = 'ecg_train'\n",
    "\n",
    "coreID = trainDataName + '_core_id.txt'\n",
    "neuronID = trainDataName + '_neuron_id.txt'\n",
    "Tm = trainDataName + \"_ts.txt\"\n",
    "\n",
    "\n",
    "reservoir_neurons_train, spike_times_train = read_Dynapse_data(coreID,neuronID,Tm)\n",
    "unit_t = 10**(-6)*second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 85.34584403038025 seconds ---\n"
     ]
    }
   ],
   "source": [
    "smoothingParameter = 2.5\n",
    "\n",
    "def func_train(i):\n",
    "    t_grid = np.linspace(spike_times_train[0], spike_times_train[-1], len(train_out))\n",
    "\n",
    "    return exp_smooth(spike_times_train[reservoir_neurons_train ==i], t_grid, alpha= smoothingParameter)\n",
    "\n",
    "\n",
    "p = Pool(20)\n",
    "start_time = time.time()\n",
    "result = p.map(func_train, range(768)) \n",
    "sicTrain = np.array(result)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nFold = 5\n",
    "\n",
    "outSpanForTrainFold = int(len(train_out)/nFold)\n",
    "\n",
    "outAllIndices = set(np.arange(len(train_out)))\n",
    "\n",
    "\n",
    "trainIndicesAllFolds = []\n",
    "validationIndicesAllFolds = []\n",
    "\n",
    "for foldnr in np.arange(nFold):\n",
    "     \n",
    "    validationIndices= list(range(foldnr * outSpanForTrainFold, (foldnr + 1) * outSpanForTrainFold))\n",
    "    trainIndices = list(outAllIndices - set(validationIndices))\n",
    "    validationIndicesAllFolds.append(validationIndices)\n",
    "    trainIndicesAllFolds.append(trainIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "avg_w 3\n",
      "binarize_thres 0.2\n",
      "smoothing parameter 2.5\n",
      "ridge parameter 60000\n",
      "------------------------------------------\n",
      "av ACC train 0.9658080067285442\n",
      "av SEN train 0.8450831936123759\n",
      "av PRE train 0.9920020243750853\n",
      "av F1 train 0.9125656208216297\n",
      "av ACC validation 0.9588625096953096\n",
      "av SEN validation 0.831574149221208\n",
      "av PRE validation 0.9666666666666668\n",
      "av F1 validation 0.8922260655458356\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "ACC_tr_all = []\n",
    "SEN_tr_all = []\n",
    "PRE_tr_all = []\n",
    "F1_tr_all = []\n",
    "ACC_ts_all = []\n",
    "SEN_ts_all = []\n",
    "PRE_ts_all = []\n",
    "F1_ts_all = []\n",
    "\n",
    "avg_w = 3\n",
    "ridgeParameter = 60000\n",
    "binarize_thres = 0.2\n",
    "\n",
    "for foldNr in np.arange(nFold):\n",
    "\n",
    "    sicTrainFold = sicTrain[:,trainIndicesAllFolds[foldNr] ]\n",
    "    sicValidationFold = sicTrain[:,validationIndicesAllFolds[foldNr] ]\n",
    "\n",
    "    outTrainFold = train_out[trainIndicesAllFolds[foldNr]]\n",
    "    outValidationFold = train_out[validationIndicesAllFolds[foldNr]]\n",
    "    rPeaks_locsTrain = rPeaks_locs_train[trainIndicesAllFolds[foldNr]]\n",
    "    rPeaks_locsValidation = rPeaks_locs_train[validationIndicesAllFolds[foldNr]]\n",
    "\n",
    "    clf = Ridge(alpha = ridgeParameter)\n",
    "    clf.fit(np.mat(sicTrainFold).T, outTrainFold)\n",
    "    train_fold_prdct = clf.predict(np.mat(sicTrainFold).T )\n",
    "    better_train_fold_prdct = better_predict(train_fold_prdct, rPeaks_locsTrain, avg_w, binarize_thres)\n",
    "    better_outTrainFold = better_predict(outTrainFold, rPeaks_locsTrain, avg_w, binarize_thres)\n",
    "\n",
    "    validation_fold_prdct = clf.predict(np.mat(sicValidationFold).T)\n",
    "    better_validation_fold_prdct = better_predict(validation_fold_prdct, rPeaks_locsValidation, avg_w, binarize_thres)\n",
    "    better_outValidationFold = better_predict(outValidationFold, rPeaks_locsValidation, avg_w, binarize_thres)\n",
    "\n",
    "    better_cnf_matrix_train = confusion_matrix(better_outTrainFold, better_train_fold_prdct)\n",
    "    better_cnf_matrix_validation = confusion_matrix(better_outValidationFold, better_validation_fold_prdct)\n",
    "\n",
    "    # confusion stats\n",
    "    ACC_tr, SEN_tr, PRE_tr, F1_tr = confusion_stat(better_cnf_matrix_train)\n",
    "    ACC_ts, SEN_ts, PRE_ts, F1_ts = confusion_stat(better_cnf_matrix_validation)\n",
    "\n",
    "#     print('Fold number', foldNr)\n",
    "#     print(ACC_tr, SEN_tr, PRE_tr, F1_tr)\n",
    "#     print(ACC_ts, SEN_ts, PRE_ts, F1_ts)\n",
    "    \n",
    "    ACC_tr_all.append(ACC_tr)\n",
    "    SEN_tr_all.append(SEN_tr)\n",
    "    PRE_tr_all.append(PRE_tr)\n",
    "    F1_tr_all.append(F1_tr)\n",
    "    ACC_ts_all.append(ACC_ts)\n",
    "    SEN_ts_all.append(SEN_ts)\n",
    "    PRE_ts_all.append(PRE_ts)\n",
    "    F1_ts_all.append(F1_ts)\n",
    "\n",
    "print('==========================================')\n",
    "print('avg_w', avg_w)\n",
    "print('binarize_thres', binarize_thres)\n",
    "print('smoothing parameter', smoothingParameter)\n",
    "print('ridge parameter', ridgeParameter)\n",
    "print('------------------------------------------')\n",
    "print('av ACC train', np.array(ACC_tr_all).mean())\n",
    "print('av SEN train', np.array(SEN_tr_all).mean())\n",
    "print('av PRE train', np.array(PRE_tr_all).mean())\n",
    "print('av F1 train', np.array(F1_tr_all).mean())\n",
    "\n",
    "print('av ACC validation', np.array(ACC_ts_all).mean())\n",
    "print('av SEN validation', np.array(SEN_ts_all).mean())\n",
    "print('av PRE validation', np.array(PRE_ts_all).mean())\n",
    "print('av F1 validation', np.array(F1_ts_all).mean())    \n",
    "print('==========================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1724664,)\n",
      "(1724664,)\n"
     ]
    }
   ],
   "source": [
    "dataName = 'ecg_test'\n",
    "\n",
    "coreID = dataName + '_core_id.txt'\n",
    "neuronID = dataName + '_neuron_id.txt'\n",
    "Tm = dataName + \"_ts.txt\"\n",
    "\n",
    "\n",
    "reservoir_neurons_test, spike_times_test = read_Dynapse_data(coreID,neuronID,Tm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothingParameter_test =  smoothingParameter\n",
    "ridgeParameter_test =  ridgeParameter\n",
    "binarize_thres_test = binarize_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 42.68375515937805 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def func_test(i):\n",
    "    t_grid = np.linspace(spike_times_test[0], spike_times_test[-1], len(test_out))\n",
    "\n",
    "    return exp_smooth(spike_times_test[reservoir_neurons_test==i], t_grid, alpha= smoothingParameter_test)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "p = Pool(20)\n",
    "start_time = time.time()\n",
    "result = p.map(func_test, range(768)) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "sicTest = np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = Ridge(alpha = ridgeParameter_test)\n",
    "clf.fit(np.mat(sicTrain).T, train_out)\n",
    "train_prdct = clf.predict(np.mat(sicTrain).T)\n",
    "test_prdct = clf.predict(np.mat(sicTest).T)\n",
    "\n",
    "\n",
    "better_train_prdct = better_predict(train_prdct, rPeaks_locs_train, avg_w, binarize_thres)\n",
    "better_test_prdct = better_predict(test_prdct, rPeaks_locs_test, avg_w, binarize_thres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "better_train_out = better_predict(train_out, rPeaks_locs_train, avg_w, binarize_thres)\n",
    "better_test_out = better_predict(test_out, rPeaks_locs_test, avg_w, binarize_thres)\n",
    "better_cnf_matrix_train = confusion_matrix(better_train_out, better_train_prdct)\n",
    "better_cnf_matrix_test = confusion_matrix(better_test_out, better_test_prdct)\n",
    "\n",
    "# confusion stats\n",
    "ACC_tr, SEN_tr, PRE_tr, F1_tr = confusion_stat(better_cnf_matrix_train)\n",
    "ACC_ts, SEN_ts, PRE_ts, F1_ts = confusion_stat(better_cnf_matrix_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============test========================\n",
      "smoothing parameter 2.5\n",
      "ridge parameter 60000\n",
      "------------------------------------------\n",
      "ACC all train 0.9650455927051672\n",
      "SEN all train 0.841726618705036\n",
      "PRE all train 0.9915254237288136\n",
      "F1  all train 0.9105058365758755\n",
      "ACC test 0.9786585365853658\n",
      "SEN test 0.875\n",
      "PRE test 1.0\n",
      "F1  test 0.9333333333333333\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "print('==============test========================')\n",
    "print('smoothing parameter', smoothingParameter)\n",
    "print('ridge parameter', ridgeParameter)\n",
    "print('------------------------------------------')\n",
    "print('ACC all train', ACC_tr)\n",
    "print('SEN all train', SEN_tr)\n",
    "print('PRE all train', PRE_tr)\n",
    "print('F1  all train', F1_tr)\n",
    "\n",
    "print('ACC test', ACC_ts)\n",
    "print('SEN test', SEN_ts)\n",
    "print('PRE test', PRE_ts)\n",
    "print('F1  test', F1_ts)    \n",
    "print('==========================================')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
